#snakemake --use-conda --cores all -s workflow/Snakefile_part1 -n
#snakemake --use-conda --cores all -s workflow/Snakefile_part2 -n
#snakemake --use-conda --cores all -s workflow/Snakefile_part3 -n

# if you know how to set up profile and path use below otherwise the previous command can apply for all users:
# Breakpoint One: preprocessing the input files
#snakemake --profile default all -s workflow/Snakefile_part1 -n
# Breakpoint Two: mcscan and dupfinder procesing
#snakemake --profile default all -s workflow/Snakefile_part2 -n
# Breakpoint Three: hsdfinder processing
#snakemake --profile default all -s workflow/Snakefile_part3 -n

# draw dag plot to have an overview of the snakemake pipeline: snakemake -s workflow/Snakefile_part1 --forceall --rulegraph | dot -Tpdf > dag_part1.pdf
# Note: Must be careful with the space and Indentation

configfile: "config.yaml"

rule all:
	input:
		expand("resources/{name}/{name}.primary.gff",
			name = config['names']),
		expand("resources/{name}/{name}.primary.fa",
			name = config['names']),
		expand("data/ncbi/{name}_primary/{name}_protein.faa",
			name = config['names']),
		expand("data/intermediateData/{name}/{name}.cds",
			name = config['names']),
	# expand("data/ncbi/{name}_primary/{name}_protein.list",
	# 	name = config['names']),
	# prepare the input gff for the mcscanx
		expand("data/intermediateData/{name}/{name}.gff",
			name = config['names']),
	# prepare the input blast db for the mcscanx			
		expand("data/ncbiDB/{name}.dmnd",
			name = config['names']),
	# prepare the input blast for the mcscanx
		expand("data/intermediateData/{name}/{name}.blast",
			name = config['names']),
	# run the DupGen_finder script
		expand("data/intermediateData/{name}_DupGen_finder/",
			name = config['names']),

# primary gff3
rule agat_gff_keep_longest_iso:
	input:
		fasta = lambda wc: config['ncbi_genomes'][wc.name]['assembly'],
		gff = "resources/{name}/{name}.gff"
	output:
		"resources/{name}/{name}.primary.gff"
	log:
		out = "log/agat/{name}.primary.out",
		err = "log/agat/{name}.primary.err"
	threads:
		 5
	resources:
		mem = 10,
		time = "2:30:0"
	conda:
		"envs/agat.yaml"
	shell:"""
agat_sp_keep_longest_isoform.pl --gff {input.gff} -o {output} \
> {log.out} \
2> {log.err}
"""


# Convert gff and fasta to protein
rule agat_primary_protein:
	input:
		fasta = lambda wc: config['ncbi_genomes'][wc.name]['assembly'],
		gff = "resources/{name}/{name}.primary.gff"
	output:
		"resources/{name}/{name}.primary.fa"
	log:
		out = "log/agat/{name}.primary.out",
		err = "log/agat/{name}.primary.err"
	threads:
		 10
	resources:
		mem = 10,
		time = "2:30:0"
	conda:
		"envs/agat.yaml"
	shell:"""
agat_sp_extract_sequences.pl \
	-g {input.gff} \
	-f {input.fasta} -p \
	-o {output} \
> {log.out} \
2> {log.err}
"""


rule agat_primary_cds:
	input:
		fasta = lambda wc: config['ncbi_genomes'][wc.name]['assembly'],
		gff = "resources/{name}/{name}.primary.gff"
	output:
		"data/intermediateData/{name}/{name}.cds"
	log:
		out = "log/agat/{name}.primary.out",
		err = "log/agat/{name}.primary.err"
	threads:
		 10
	params:
		dir = "data/intermediateData/{name}"
	resources:
		mem = 10,
		time = "2:30:0"
	conda:
		"envs/agat.yaml"
	shell:"""
mkdir -p {params.dir}; \
agat_sp_extract_sequences.pl \
	-g {input.gff} \
	-f {input.fasta} -t cds \
	-o {output} \
> {log.out} \
2> {log.err}
"""


rule prep_primary_protein_short_name:
	input:
		#lambda wc: config['genomes'][wc.name]['proteins_fasta']
		"resources/{name}/{name}.primary.fa"
	output:
		"data/ncbi/{name}_primary/{name}_protein.faa"
	threads:
		 5
	resources:
		mem = 25
	params:
		dir = "data/ncbi/{name}_primary"
	log: 
		out = "log/prep_primary_protein/{name}.out",
		err = "log/prep_primary_protein/{name}.err"
	shell:"""
mkdir -p {params.dir}; \
sed 's/^>.* gene=\\(.*\\) seq_id=.*/>\\1/g' {input} > {output} \
2> {log.out} \
3> {log.err}
"""


rule gff_to_MCScanX_gff:
	input:
		gff = "resources/{name}/{name}.primary.gff"
	output:
		MCScanX_gff = "data/intermediateData/{name}/{name}.gff"
	log:
		err = "log/gff_to_MCScanX_gff/{name}.err"
	conda:
		"envs/bedops.yaml"
	threads:
		1
	resources:
		mem = 4,
		time = "0:10:0"
	shell: """
cat {input.gff} \
| grep -v '^#' \
| awk '$3 == "gene"'|sed 's/gene-//g' \
| gff2bed \
| awk 'BEGIN {{OFS="\t"}} {{print $1,$4,$2,$3}}' \
> {output.MCScanX_gff} \
2> {log.err}
"""



# Waiting at most 60 seconds for missing files
rule diamond_db_mcscanx:
	input:
		"data/ncbi/{name}_primary/{name}_protein.faa"
	output:
		"data/ncbiDB/{name}.dmnd"
	threads:
		1
	resources:
		mem = 2
	params:
		dir1 = "data/ncbiDB",
		db_name_dir = "data/ncbiDB/{name}",
		protein = "data/ncbi/{name}_primary/{name}_protein.faa"
	conda: 
		"envs/diamond.yaml"
	log: 
		out = "log/diamond_db_mcscanx/{name}.out",
		err = "log/diamond_db_mcscanx/{name}.err"
	shell:"""
	sleep 10s; \
	mkdir -p {params.dir1}; \
	diamond makedb \
		--in {params.protein} \
		-d {params.db_name_dir} \
> {log.out} \
2> {log.err}
"""

# note: --max-target-seqs parameter will impact how many candidate duplicates will be detected
rule diamond_mcscanx:
	input:
		protein = "data/ncbi/{name}_primary/{name}_protein.faa",
		database = "data/ncbiDB/{name}.dmnd"
	output:
		"data/intermediateData/{name}/{name}.blast"
	threads:
		4
	resources:
		mem = 20
	params:
		db_name_dir = "data/ncbiDB/{name}",
		db_title = "{name}",
		protein = "data/ncbi/{name}_primary/{name}_protein.faa"
	conda: 
		"envs/diamond.yaml"	
	log: 
		out = "log/diamond_mcscanx/{name}.out",
		err = "log/diamond_mcscanx/{name}.err"
	shell:"""
		diamond blastp \
		-d {params.db_name_dir} \
		-q {params.protein} \
		-o {output} \
		-e 1e-10 \
		-f 6 \
		-p {threads} \
		--sensitive \
		--max-target-seqs 5 \
> {log.out} \
2> {log.err}	
"""

# {wc.name} and {name} cannot be put together
rule DupGen_finder_diamond:
	input:
		protein = "data/ncbi/{name}_primary/{name}_protein.faa",
		database = lambda wc: "data/ncbiDB/" + config['ncbi_genomes'][wc.name]['outgroup'] + ".dmnd"
	output:
		directory("data/intermediateData/{name}_DupGen_finder/")
	threads:
		4
	resources:
		mem = 20
	params:
		db_name_dir = lambda wc: "data/ncbiDB/" + config['ncbi_genomes'][wc.name]['outgroup'],
		db_title = lambda wc: "config['ncbi_genomes'][wc.name]['outgroup']",
		protein = "data/ncbi/{name}_primary/{name}_protein.faa",
		out_name = lambda wc: "data/intermediateData/" + wc.name + "_DupGen_finder/" + wc.name + "_" + config['ncbi_genomes'][wc.name]['outgroup'] + ".blast",
		dir = "data/intermediateData/{name}_DupGen_finder/"
	conda:
		"envs/diamond.yaml"
	log:
		out = "log/DupGen_finder_diamond/{name}.out",
		err = "log/DupGen_finder_diamond/{name}.err"
	shell:"""
		mkdir -p {params.dir}; \
		diamond blastp \
		-d {params.db_name_dir} \
		-q {params.protein} \
		-o {params.out_name} \
		-e 1e-10 \
		-f 6 \
		-p {threads} \
		--sensitive \
		--max-target-seqs 5 \
> {log.out} \
2> {log.err}	
"""

