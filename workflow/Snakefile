#snakemake --use-conda --cores all

# if you know how to set up profile and path use below otherwise the previous command can apply for all users:
#snakemake --profile default all

# draw dag plot to have an overview of the snakemake pipeline:  snakemake --forceall --rulegraph | dot -Tpdf > dag.pdf

configfile: "config.yaml"

rule all:
	input:
	#preprocess fasta
		expand("data/preprocess_fasta/{sample}.fa",
			sample = config['samples']),
	#diamond_db
		expand("results/{sample}/{sample}.dmnd",
			sample = config['samples']),
	#diamond blastp
		expand("results/{sample}/diamond/{sample}.txt",
			sample = config['samples']),
	
	#interproscan
	#	expand("results/{sample}/interproscan/{sample}.fa.tsv",
	#		sample = config['samples']),

	#kegg blastkoala
	
	# HSDfinder preprocess
		expand("results/{sample}/diamond/{sample}.preprocess.txt",
			sample = config['samples']),
	
	#hsdfinder
		expand("results/{sample}/hsdfinder/{sample}.{HSD_identity}_{HSD_variance}.txt",
			sample = config['samples'],HSD_identity = config['HSD_identity'],HSD_variance = config['HSD_variance']),
	#kegg category
		expand("results/{sample}/kegg/{sample}.{HSD_identity}_{HSD_variance}.kegg.txt",
			sample = config['samples'],HSD_identity = config['HSD_identity'],HSD_variance = config['HSD_variance']),
	
	#hsdecipher statistcs
		expand("results/{sample}/hsdecipher/stats/{sample}.stat.txt",
			sample = config['samples']),
	
	#hsdecipher category
		expand("results/{sample}/hsdecipher/stats/{sample}.category.txt",
			sample = config['samples']),
	
	#hsdecipher merge statistics
		expand("results/{sample}/hsdecipher/stats/{sample}.complete.stats.txt",
			sample = config['samples']),
	
	#hsdecipher batch run
		expand("results/{sample}/hsdecipher/batch_run/{sample}.batch_run.txt",
			sample = config['samples']),
	
	#hsdecipher heatmap intra species
		expand("results/{sample}/hsdecipher/heatmap/{sample}.output_heatmap.eps",
			sample = config['samples']),
			
	#hsdecipher heatmap inter species	
		expand("results/heatmap_inter/HSD/{sample}.batch_run.txt",
			sample = config['samples']),
		expand("results/heatmap_inter/ko/{sample}.ko.txt",
			sample = config['samples'])

rule preprocess_fasta:
	input:
		lambda wc: config['genomes'][wc.sample]['proteins']
	output:
		"data/preprocess_fasta/{sample}.fa"
	threads:
		 1
	resources:
		mem = 2
	params:
		dir = "data/preprocess_fasta"
	log: 
		out = "log/preprocess_fasta/{sample}.out",
		err = "log/preprocess_fasta/{sample}.err"
	shell:"""
	mkdir -p {params.dir}; \
	awk '{{print $1}}' {input} \
> {output}
2> {log.out} \
3> {log.err}
"""


rule diamond_db:
	input:
		"data/preprocess_fasta/{sample}.fa"
	output:
		"results/{sample}/{sample}.dmnd"
	threads:
		 1
	resources:
		mem = 2
	params:
		db_name_dir = "results/{sample}/{sample}"
	conda: 
		"envs/diamond.yaml"
	log: 
		out = "log/diamond_db/{sample}.out",
		err = "log/diamond_db/{sample}.err"
	shell:"""
	diamond makedb \
		--in {input} \
		-d {params.db_name_dir} \
> {log.out} \
2> {log.err}
"""

rule diamond:
	input:
		protein = "data/preprocess_fasta/{sample}.fa",
		database = "results/{sample}/{sample}.dmnd"
	output:
		"results/{sample}/diamond/{sample}.txt"
	threads:
		 2
	resources:
		mem = 4
	params:
		db_name_dir = "results/{sample}/{sample}",
		db_title = "{sample}" 
	conda: 
		"envs/diamond.yaml"	
	log: 
		out = "log/diamond/{sample}.out",
		err = "log/diamond/{sample}.err"
	shell:"""
		diamond blastp \
		-d {params.db_name_dir} \
		-q {input.protein} \
		-o {output} \
		-e 1e-10 \
		-f 6 \
		-p {threads} \
		--sensitive \
> {log.out} \
2> {log.err}	
"""

# conda not available for interproscan version above 5.59, which can cause hmmpress error with old version. 
# rule interproscan:
# 	input:
# 		"results/{sample}/{sample}.fa"
# 	output:
# 		"results/{sample}/interproscan/{sample}.fa.tsv"
# 	threads:
# 		 10
# 	resources:
# 		mem = 25
# 	params:
# 		db_name_dir = "results/{sample}/interproscan"
# 	conda: 
# 		"envs/interproscan.yaml"
# 	log: 
# 		out = "log/interproscan/{sample}.out",
# 		err = "log/interproscan/{sample}.err"
# 	shell:"""
# 	interproscan.sh \
# 		-i {input} \
# 		-f tsv \
# 		-dp \
# 		-b {params.db_name_dir} \
# > {log.out} \
# 2> {log.err}	
# """

# In rare case, like the key error which is due to in your new blast all-against-all file, you are still missing the length info lines like below if you search it. The Unix command below will help.please refer to https://github.com/zx0223winner/HSDFinder/issues/2
rule HSDFinder_preprocess:
	input:
		tabular = "results/{sample}/diamond/{sample}.txt",
		protein = "data/preprocess_fasta/{sample}.fa"
	output:
		"results/{sample}/diamond/{sample}.preprocess.txt"
	threads:
		 1
	resources:
		mem = 2
	params:
		dir = config['HSDFinder']
	conda:
		"envs/hsdfinder.yaml"
	log: 
		out = "log/hsdfinder_preprocess/{sample}.out",
		err = "log/hsdfinder_preprocess/{sample}.err"
	shell:"""
	(awk '/^>/{{if (l!="") print l; print; l=0; next}}{{l+=length($0)}}END{{print l}}' \
		{input.protein} \
		|paste - - \
		|sed 's/>//g' \
		|awk -F'\t' '{{print $1"\t"$1"\t"100"\t"$2}}' \
		|cat ;\
		cat {input.tabular}) \
		|cat \
> {output}
2> {log.out} \
3> {log.err}	
"""

rule HSDFinder:
	input:
		tabular = "results/{sample}/diamond/{sample}.preprocess.txt",
		Interproscan = lambda wc: config['genomes'][wc.sample]['interproscan']
	output:
		"results/{sample}/hsdfinder/{sample}.{HSD_identity}_{HSD_variance}.txt"
	threads:
		 1
	resources:
		mem = 2
	params:
		dir = config['HSDFinder'],
		identity = "{HSD_identity}",
		variance = "{HSD_variance}" 
	conda:
		"envs/hsdfinder.yaml"
	log: 
		out = "log/hsdfinder/{sample}_{HSD_identity}_{HSD_variance}.out",
		err = "log/hsdfinder/{sample}_{HSD_identity}_{HSD_variance}.err"
	shell:"""
	hsdfinder \
		-i {input.tabular} \
		-p {params.identity} \
		-l {params.variance} \
		-f {input.Interproscan} \
		-t Pfam \
		-o {output} \
> {log.out} \
2> {log.err}	
"""
# python3 $PWD{params.dir}HSDFinder.py

rule KEGG:
	input:
		HSD_result = "results/{sample}/hsdfinder/{sample}.{HSD_identity}_{HSD_variance}.txt",
		KEGG = lambda wc: config['genomes'][wc.sample]['KEGG']
	output:
		"results/{sample}/kegg/{sample}.{HSD_identity}_{HSD_variance}.kegg.txt"
	threads:
		 1
	resources:
		mem = 2
	params:
		dir = config['HSDFinder'],
		identity = "{HSD_identity}",
		variance = "{HSD_variance}" ,
		species_name = "{sample}"
	conda:
		"envs/hsdfinder.yaml"
	log: 
		out = "log/kegg/{sample}_{HSD_identity}_{HSD_variance}.out",
		err = "log/kegg/{sample}_{HSD_identity}_{HSD_variance}.err"
	shell:"""
	python3 $PWD{params.dir}HSD_to_KEGG.py \
		-i {input.HSD_result} \
		-k {input.KEGG} \
		-n {params.species_name}\
		-o {output} \
> {log.out} \
2> {log.err}	
"""




rule HSDecipher_stastics:
	input:
		"results/{sample}/hsdfinder/{sample}.90_10.txt"
	output:
		"results/{sample}/hsdecipher/stats/{sample}.stat.txt"
	threads:
		 1
	resources:
		mem = 2
	params:
		dir = config['HSDecipher'],
		HSD_dir = "results/{sample}/hsdfinder/",
		HSD_file_format = "txt"
	conda:
		"envs/hsdfinder.yaml"
	log: 
		out = "log/hsdecipher_stastics/{sample}.out",
		err = "log/hsdecipher_stastics/{sample}.err"
	shell:"""
	python3 $PWD{params.dir}HSD_statistics.py \
		{params.HSD_dir} \
		{params.HSD_file_format} \
		{output} \
> {log.out} \
2> {log.err}	
"""

rule HSDecipher_category:
	input:
		"results/{sample}/hsdfinder/{sample}.90_10.txt"
	output:
		"results/{sample}/hsdecipher/stats/{sample}.category.txt"
	threads:
		 1
	resources:
		mem = 2
	params:
		dir = config['HSDecipher'],
		HSD_dir = "results/{sample}/hsdfinder/",
		HSD_file_format = "txt"
	conda:
		"envs/hsdfinder.yaml"
	log: 
		out = "log/hsdecipher_stats/{sample}.out",
		err = "log/hsdecipher_stats/{sample}.err"
	shell:"""
	python3 $PWD{params.dir}HSD_categories.py \
		{params.HSD_dir} \
		{params.HSD_file_format} \
		{output} \
> {log.out} \
2> {log.err}	
"""

rule merge_stastics:
	input:
		stat = "results/{sample}/hsdecipher/stats/{sample}.stat.txt",
		category = "results/{sample}/hsdecipher/stats/{sample}.category.txt"
	output:
		"results/{sample}/hsdecipher/stats/{sample}.complete.stats.txt"
	threads:
		 1
	resources:
		mem = 2
	log: 
		out = "log/hsdecipher_stastics_merge/{sample}.out",
		err = "log/hsdecipher_stastics_merge/{sample}.err"
	shell:"""
	paste -d"\t" \
	{input.stat} \
	{input.category} \
	> {output} \
2> {log.out} \
3> {log.err}	
"""

rule HSDecipher_batch_run:
	input:
		"results/{sample}/hsdfinder/{sample}.90_10.txt"
	output:
		"results/{sample}/hsdecipher/batch_run/{sample}.batch_run.txt"
	threads:
		 1
	resources:
		mem = 2
	params:
		dir = config['HSDecipher'],
		HSD_dir = "results/{sample}/hsdfinder",
		HSD_batch_run_dir = "results/{sample}/hsdecipher/batch_run",
		species_name = "{sample}"
	conda:
		"envs/hsdecipher.yaml"
	log: 
		out = "log/HSDecipher_batch_run/{sample}.out",
		err = "log/HSDecipher_batch_run/{sample}.err"
	shell:"""
	mkdir -p {params.HSD_batch_run_dir}/{params.species_name}; \
	cp {params.HSD_dir}/* {params.HSD_batch_run_dir}/{params.species_name}/; \
	python3 $PWD{params.dir}HSD_batch_run.py \
		-i {params.HSD_batch_run_dir}; \
	rm -r {params.HSD_batch_run_dir}/{params.species_name} \
> {log.out} \
2> {log.err}	
"""

# sleep 30s is to wait for the previous step to yield {sample}.batch_run.txt file, it is depending on the size of your file and speed of CPU. 30s should be enough for genome size likeArabidopsis thalina
rule HSDecipher_heatmap_intra_species:
	input:
		HSD_file = "results/{sample}/hsdfinder/{sample}.90_10.txt",
		KEGG = lambda wc: config['genomes'][wc.sample]['KEGG']
	output:
		"results/{sample}/hsdecipher/heatmap/{sample}.output_heatmap.eps"
	threads:
		 1
	resources:
		mem = 2
	conda: 
		"envs/hsdecipher.yaml"
	params:
		dir = config['HSDecipher'],
		HSD_dir = "results/{sample}/hsdfinder",
		HSD_heatmap_dir = "results/{sample}/hsdecipher/heatmap",
		species_name = "{sample}",
		batch_run = "results/{sample}/hsdecipher/batch_run/{sample}.batch_run.txt",
		r = config['heatmap_hight'],
		c = config['heatmap_width'],
		ko_dir = "results/{sample}/hsdecipher/heatmap/ko",
		heatmap = "{sample}.output_heatmap.eps",
		tabular = "{sample}.output_heatmap.tsv"
	log: 
		out = "log/HSDecipher_heatmap/{sample}.out",
		err = "log/HSDecipher_heatmap/{sample}.err"
	shell:"""
	mkdir -p {params.HSD_heatmap_dir}/{params.species_name}; \
	mkdir -p {params.ko_dir}; \
	sleep 60s; \
	cp {input.KEGG} {params.ko_dir}/; \
	cp {params.HSD_dir}/* {params.HSD_heatmap_dir}/{params.species_name}/; \
	cp {params.batch_run} {params.HSD_heatmap_dir}/{params.species_name}/; \
	hsdecipher \
	-f {params.HSD_heatmap_dir}/{params.species_name} \
	-k {params.ko_dir} \
	-r {params.r} \
	-c {params.c}; \
	rm -r {params.ko_dir}; \
	rm -r {params.HSD_heatmap_dir}/{params.species_name}; \
	mv {params.heatmap} {params.HSD_heatmap_dir}; \
	mv {params.tabular} {params.HSD_heatmap_dir}; \
> {log.out} \
2> {log.err}
"""
# python3 $PWD{params.dir}HSD_heatmap.py

rule HSDecipher_heatmap_inter_species_prepare:
	input:
		HSD_file = "results/{sample}/hsdfinder/{sample}.90_10.txt",
		KEGG = lambda wc: config['genomes'][wc.sample]['KEGG']
	output:
		"results/heatmap_inter/HSD/{sample}.batch_run.txt",
		"results/heatmap_inter/ko/{sample}.ko.txt"
	threads:
		1
	resources:
		mem =2
	conda:
		"envs/hsdecipher.yaml"
	params:
		dir = config['HSDecipher'],
		batch_run = "results/{sample}/hsdecipher/batch_run/{sample}.batch_run.txt",
		HSD_heatmap_dir = "results/heatmap_inter/HSD",
		ko_heatmap_dir = "results/heatmap_inter/ko",
		ko_dir = "data/",
		r = config['heatmap_hight'],
		c = config['heatmap_width'],
		heatmap = "HSD.output_heatmap.eps",
		tabular = "HSD.output_heatmap.tsv",
		HSD_heatmap = "results/heatmap_inter"
	log:
		out = "log/heatmap_inter/{sample}.out",
		err = "log/heatmap_inter/{sample}.err"
	shell:"""
		mkdir -p {params.HSD_heatmap_dir}; \
		mkdir -p {params.ko_heatmap_dir}; \
		sleep 30s; \
		cp {params.batch_run} {params.HSD_heatmap_dir}; \
		cp {input.KEGG} {params.ko_heatmap_dir}; \
		hsdecipher \
		-f {params.HSD_heatmap_dir}\
		-k {params.ko_heatmap_dir} \
		-r {params.r} \
		-c {params.c}; \
		mv {params.heatmap} {params.HSD_heatmap}||true; \
		mv {params.tabular} {params.HSD_heatmap}||true; \
> {log.out} \
2> {log.err}	
"""
#python3 $PWD{params.dir}HSD_heatmap.py
